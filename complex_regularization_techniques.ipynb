{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. tiny-imagenet\n",
    "#   (a) Dataset Summary - Tiny ImageNet contains 100,000+ images of 200 classes (500 for each class) downsized to 64Ã—64 colored images. Each class has 500 training images, 50 validation images, and 50 test images.\n",
    "#   (b) Data Feature Dimensions -\n",
    "#       i. Image: A PIL.Image.Image object containing the image.\n",
    "#       ii. Label: an int classification label. -1 for the test set as the labels are missing. Check classes.py for the map of numbers and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Techniques for Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. L2 Regularization - modifies the loss function. Applied to both datasets.\n",
    "# 2. Data Augmentation - modifies the data. For Dataset1 we plan to use RandomErasing\n",
    "#              - RandomErasing is concerned about removing and randomly adding information on the\n",
    "#               blank space, such as noise. For Dataset2 we plan to use Random Synonym Replacement -\n",
    "#               Random Synonym Replacement is concerned about removing and replacing with a synonym.\n",
    "# 3. MaxDropout - modifies training approach. Applied to both datasets.\n",
    "# 4. Ensemble Regularization 1 - applying RandomErasing and MaxDropout together. Applied to Dataset1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. nltk-brown + nltk-treebank + nltk-conll2000\n",
    "#   (a) Dataset Summary - The combination of these 3 datasets gives us a large corpus of\n",
    "#                       textual data that can be used for training a model that performs sequence labeling with\n",
    "#                       a total size of 72,000+ tagged sentences. The nltk library takes the base dataset and\n",
    "#                       performs tokenization to prepare it for the task of sequence labeling.\n",
    "#   (b) Data Feature Dimensions -\n",
    "#         i. Input Sequence - A sentence in english.\n",
    "#        ii. Output Sequence - POS tags of each word of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization Techniques for Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. L2 Regularization - modifies the loss function. Applied to both datasets.\n",
    "# 2. Data Augmentation - modifies the data. For Dataset1 we plan to use RandomErasing\n",
    "#       - RandomErasing is concerned about removing and randomly adding information on the\n",
    "#         blank space, such as noise. For Dataset2 we plan to use Random Synonym Replacement -\n",
    "#         Random Synonym Replacement is concerned about removing and replacing with a synonym.\n",
    "# 3. MaxDropout - modifies training approach. Applied to both datasets.\n",
    "# 4. Ensemble Regularization 2 - applying RandomSynonymReplacement and MaxDropout together. Applied to Dataset2."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
