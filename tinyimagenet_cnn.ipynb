{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: torch in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (0.16.0)\n",
      "Collecting pytorch-ignite\n",
      "  Obtaining dependency information for pytorch-ignite from https://files.pythonhosted.org/packages/36/94/a31545339afd94b888dcffc261495815f4b56ef5299b71b0299d4f09704e/pytorch_ignite-0.4.13-py3-none-any.whl.metadata\n",
      "  Downloading pytorch_ignite-0.4.13-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tensorboardX\n",
      "  Obtaining dependency information for tensorboardX from https://files.pythonhosted.org/packages/44/71/f3e7c9b2ab67e28c572ab4e9d5fa3499e0d252650f96d8a3a03e26677f53/tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: tensorboard in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (2.16.2)\n",
      "Collecting opendatasets\n",
      "  Obtaining dependency information for opendatasets from https://files.pythonhosted.org/packages/00/e7/12300c2f886b846375c78a4f32c0ae1cd20bdcf305b5ac45b8d7eceda3ec/opendatasets-0.1.22-py3-none-any.whl.metadata\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting efficientnet-pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: requests in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: packaging in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from pytorch-ignite) (23.1)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboardX) (4.23.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.59.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from tensorboard) (2.2.3)\n",
      "Requirement already satisfied: tqdm in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from opendatasets) (4.65.0)\n",
      "Collecting kaggle (from opendatasets)\n",
      "  Downloading kaggle-1.6.8.tar.gz (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from opendatasets) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (1.26.18)\n",
      "Requirement already satisfied: bleach in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from kaggle->opendatasets) (4.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: webencodings in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /Users/shreyasmalewar/anaconda3/lib/python3.11/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Downloading pytorch_ignite-0.4.13-py3-none-any.whl (272 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.4/272.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: efficientnet-pytorch, kaggle\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=da1d6744de29b7afbba55ee9384174d4fd7f254e9ce17b35a7bd5f6da0de954f\n",
      "  Stored in directory: /Users/shreyasmalewar/Library/Caches/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.8-py3-none-any.whl size=111967 sha256=23b9e594c49cc6e3f9c6da8f23a91f9d8136d0e806b7c8c076b3059b47c4c6e5\n",
      "  Stored in directory: /Users/shreyasmalewar/Library/Caches/pip/wheels/8c/fe/8c/71a8dd0e02634fd0e4ba4abaaf2d4a6049cccff349625331e1\n",
      "Successfully built efficientnet-pytorch kaggle\n",
      "Installing collected packages: tensorboardX, kaggle, pytorch-ignite, opendatasets, efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1 kaggle-1.6.8 opendatasets-0.1.22 pytorch-ignite-0.4.13 tensorboardX-2.6.2.2\n"
     ]
    }
   ],
   "source": [
    "# Install Python packages\n",
    "! pip install numpy torch torchvision pytorch-ignite tensorboardX tensorboard opendatasets efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import models, datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall\n",
    "from ignite.handlers import LRScheduler, ModelCheckpoint, global_step_from_engine\n",
    "from ignite.contrib.handlers import ProgressBar, TensorboardLogger\n",
    "import ignite.contrib.engines.common as common\n",
    "\n",
    "import opendatasets as od\n",
    "import os\n",
    "from random import randint\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "# Define device to use (CPU or GPU). CUDA = GPU support for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-30 19:50:04--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
      "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248100043 (237M) [application/zip]\n",
      "Saving to: ‘tiny-imagenet-200.zip’\n",
      "\n",
      "tiny-imagenet-200.z 100%[===================>] 236.61M  7.94MB/s    in 37s     \n",
      "\n",
      "2024-03-30 19:50:41 (6.41 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve data directly from Stanford data source\n",
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip -qq 'tiny-imagenet-200.zip'\n",
    "DATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "VALID_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to display single or a batch of sample images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def show_batch(dataloader):\n",
    "    for images, labels in dataloader:\n",
    "        imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n",
    "    \n",
    "def show_image(dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = dataiter.next()\n",
    "    random_num = randint(0, len(images)-1)\n",
    "    imshow(images[random_num])\n",
    "    label = labels[random_num]\n",
    "    print(f'Label: {label}, Shape: {images[random_num].shape}')\n",
    "\n",
    "# Setup function to create dataloaders for image datasets\n",
    "def generate_dataloader(data, name, transform):\n",
    "    if data is None: \n",
    "        return None\n",
    "    \n",
    "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
    "    # loader where images are in format root/label/filename\n",
    "    # See https://pytorch.org/vision/stable/datasets.html\n",
    "    if transform is None:\n",
    "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(data, transform=transform)\n",
    "\n",
    "    # Set options for device\n",
    "    if use_cuda:\n",
    "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
    "    else:\n",
    "        kwargs = {}\n",
    "    \n",
    "    # Wrap image dataset (defined above) in dataloader \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                        shuffle=(name==\"train\"), \n",
    "                        **kwargs)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Class</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>H</th>\n",
       "      <th>W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val_0.JPEG</td>\n",
       "      <td>n03444034</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val_1.JPEG</td>\n",
       "      <td>n04067472</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val_2.JPEG</td>\n",
       "      <td>n04070727</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val_3.JPEG</td>\n",
       "      <td>n02808440</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val_4.JPEG</td>\n",
       "      <td>n02808440</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         File      Class   X   Y   H   W\n",
       "0  val_0.JPEG  n03444034   0  32  44  62\n",
       "1  val_1.JPEG  n04067472  52  55  57  59\n",
       "2  val_2.JPEG  n04070727   4   0  60  55\n",
       "3  val_3.JPEG  n02808440   3   3  63  63\n",
       "4  val_4.JPEG  n02808440   9  27  63  48"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unlike training folder where images are already arranged in sub folders based \n",
    "# on their labels, images in validation folder are all inside a single folder. \n",
    "# Validation folder comes with images folder and val_annotations txt file. \n",
    "# The val_annotation txt file comprises 6 tab separated columns of filename, \n",
    "# class label, x and y coordinates, height, and width of bounding boxes\n",
    "val_data = pd.read_csv(f'{VALID_DIR}/val_annotations.txt', \n",
    "                       sep='\\t', \n",
    "                       header=None, \n",
    "                       names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_0.JPEG': 'n03444034',\n",
       " 'val_1.JPEG': 'n04067472',\n",
       " 'val_2.JPEG': 'n04070727',\n",
       " 'val_3.JPEG': 'n02808440',\n",
       " 'val_4.JPEG': 'n02808440',\n",
       " 'val_5.JPEG': 'n04399382',\n",
       " 'val_6.JPEG': 'n04179913',\n",
       " 'val_7.JPEG': 'n02823428',\n",
       " 'val_8.JPEG': 'n04146614',\n",
       " 'val_9.JPEG': 'n02226429'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create separate validation subfolders for the validation images based on\n",
    "# their labels indicated in the val_annotations txt file\n",
    "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
    "\n",
    "# Open and read val annotations text file\n",
    "fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
    "data = fp.readlines()\n",
    "\n",
    "# Create dictionary to store img filename (word 0) and corresponding\n",
    "# label (word 1) for every line in the txt file (as key value pair)\n",
    "val_img_dict = {}\n",
    "for line in data:\n",
    "    words = line.split('\\t')\n",
    "    val_img_dict[words[0]] = words[1]\n",
    "fp.close()\n",
    "\n",
    "# Display first 10 entries of resulting val_img_dict dictionary\n",
    "{k: val_img_dict[k] for k in list(val_img_dict)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subfolders (if not present) for validation images based on label ,\n",
    "# and move images into the respective folders\n",
    "for img, folder in val_img_dict.items():\n",
    "    newpath = (os.path.join(val_img_dir, folder))\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    if os.path.exists(os.path.join(val_img_dir, img)):\n",
    "        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n00001740': 'entity',\n",
       " 'n00001930': 'physical entity',\n",
       " 'n00002137': 'abstraction',\n",
       " 'n00002452': 'thing',\n",
       " 'n00002684': 'object',\n",
       " 'n00003553': 'whole',\n",
       " 'n00003993': 'congener',\n",
       " 'n00004258': 'living thing',\n",
       " 'n00004475': 'organism',\n",
       " 'n00005787': 'benthos',\n",
       " 'n00005930': 'dwarf',\n",
       " 'n00006024': 'heterotroph',\n",
       " 'n00006150': 'parent',\n",
       " 'n00006269': 'life',\n",
       " 'n00006400': 'biont',\n",
       " 'n00006484': 'cell',\n",
       " 'n00007347': 'causal agent',\n",
       " 'n00007846': 'person',\n",
       " 'n00015388': 'animal',\n",
       " 'n00017222': 'plant'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save class names (for corresponding labels) as dict from words.txt file\n",
    "class_to_name_dict = dict()\n",
    "fp = open(os.path.join(DATA_DIR, 'words.txt'), 'r')\n",
    "data = fp.readlines()\n",
    "for line in data:\n",
    "    words = line.strip('\\n').split('\\t')\n",
    "    class_to_name_dict[words[0]] = words[1].split(',')[0]\n",
    "fp.close()\n",
    "\n",
    "# Display first 20 entries of resulting dictionary\n",
    "{k: class_to_name_dict[k] for k in list(class_to_name_dict)[:20]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define image preprocessing transofrmations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation sequence for image pre-processing\n",
    "# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n",
    "# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n",
    "# std=[0.229, 0.224, 0.225])\n",
    "preprocess_transform = T.Compose([\n",
    "                T.Resize(256), # Resize images to 256 x 256\n",
    "                T.CenterCrop(224), # Center crop image\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                # T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # \n",
    "])\n",
    "\n",
    "preprocess_transform_pretrain = T.Compose([\n",
    "                T.Resize(256), # Resize images to 256 x 256\n",
    "                T.CenterCrop(224), # Center crop image\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),  # Converting cropped images to tensors\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for data loaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=preprocess_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display batch of training set images\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train loader for pre-trained models (normalized based on specific requirements)\n",
    "train_loader_pretrain = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                  transform=preprocess_transform_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for validation data (depending if model is pretrained)\n",
    "val_loader = generate_dataloader(val_img_dir, \"val\",\n",
    "                                 transform=preprocess_transform)\n",
    "\n",
    "val_loader_pretrain = generate_dataloader(val_img_dir, \"val\",\n",
    "                                 transform=preprocess_transform_pretrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /Users/shreyasmalewar/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n",
      "100%|██████████| 47.1M/47.1M [00:02<00:00, 17.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n",
      "Begin training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645096fbfa1e47968785da22e499781c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/1563]   0%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define model architecture (using efficientnet-b3 version)\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b3', num_classes=200)\n",
    "\n",
    "# Move model to designated device (Use GPU when on Colab)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define hyperparameters and settings\n",
    "lr = 0.001  # Learning rate\n",
    "num_epochs = 3  # Number of epochs\n",
    "log_interval = 300  # Number of iterations before logging\n",
    "\n",
    "# Set loss function (categorical Cross Entropy Loss)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer (using Adam as default)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Setup pytorch-ignite trainer engine\n",
    "trainer = create_supervised_trainer(model, optimizer, loss_func, device=device)\n",
    "\n",
    "# Add progress bar to monitor model training\n",
    "ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"Batch Loss\": x})\n",
    "\n",
    "# Define evaluation metrics\n",
    "metrics = {\n",
    "    \"accuracy\": Accuracy(), \n",
    "    \"loss\": Loss(loss_func),\n",
    "}\n",
    "\n",
    "# Setup pytorch-ignite evaluator engines. We define two evaluators as they do\n",
    "# not have exactly similar roles. `evaluator` will save the best model based on \n",
    "# validation score, whereas `train_evaluator` logs metrics on training set only\n",
    "\n",
    "# Evaluator for training data\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "# Evaluator for validation data\n",
    "evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "# Display message to indicate start of training\n",
    "@trainer.on(Events.STARTED)\n",
    "def start_message():\n",
    "    print(\"Begin training\")\n",
    "\n",
    "# Log results from every batch\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_batch(trainer):\n",
    "    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n",
    "    print(\n",
    "        f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n",
    "        f\"Batch {batch} / {trainer.state.epoch_length}: \"\n",
    "        f\"Loss: {trainer.state.output:.3f}\"\n",
    "    )\n",
    "\n",
    "# Evaluate and print training set metrics\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_loss(trainer):\n",
    "    print(f\"Epoch [{trainer.state.epoch}] - Loss: {trainer.state.output:.2f}\")\n",
    "    train_evaluator.run(train_loader_pretrain)\n",
    "    epoch = trainer.state.epoch\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(f\"Train - Loss: {metrics['loss']:.3f}, \"\n",
    "          f\"Accuracy: {metrics['accuracy']:.3f} \"\n",
    "          )\n",
    "\n",
    "# Evaluate and print validation set metrics\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_loss(trainer):\n",
    "    evaluator.run(val_loader_pretrain)\n",
    "    epoch = trainer.state.epoch\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation - Loss: {metrics['loss']:.3f}, \"\n",
    "          f\"Accuracy: {metrics['accuracy']:.3f}\"\n",
    "          )\n",
    "    print()\n",
    "    print(\"-\" * 60)\n",
    "    print()\n",
    "\n",
    "# Sets up checkpoint handler to save best n model(s) based on validation accuracy metric\n",
    "common.save_best_model_by_val_score(\n",
    "          output_path=\"best_models\",\n",
    "          evaluator=evaluator,\n",
    "          model=model,\n",
    "          metric_name=\"accuracy\",\n",
    "          n_saved=1,\n",
    "          trainer=trainer,\n",
    "          tag=\"val\"\n",
    ")\n",
    "\n",
    "# Define a Tensorboard logger\n",
    "tb_logger = TensorboardLogger(log_dir=\"logs\")\n",
    "\n",
    "# Using common module to setup tb logger (Alternative method)\n",
    "# tb_logger = common.setup_tb_logging(\"tb_logs\", trainer, optimizer, evaluators=evaluator)\n",
    "\n",
    "# Attach handler to plot trainer's loss every n iterations\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=log_interval),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"Batch Loss\": loss},\n",
    ")\n",
    "\n",
    "# Attach handler to dump evaluator's metrics every epoch completed\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.run(train_loader_pretrain, max_epochs=num_epochs)\n",
    "\n",
    "# Close Tensorboard\n",
    "tb_logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
